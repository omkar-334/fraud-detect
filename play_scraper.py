import json
import os

from playwright.sync_api import sync_playwright
from serpapi import GoogleSearch

from data import add_info, get_app_details, search_apps

# https://watch.appfollow.io/apps/my-first-workspace/aso/rankings/android/in/finance?date=2025-04-03


def serpapi_category_apps(category: str, country: str):
    all_apps = []
    # no reliable source for the types of charts, only one is given in serpapi docs, rest are generated by claude.
    charts = [
        "topselling_free",
        "topselling_paid",
        "topgrossing",
        "movers_shakers",
        "topselling_new_free",
        "topselling_new_paid",
    ]
    for chart in charts:
        params = {"engine": "google_play", "gl": country, "apps_category": category, "chart": chart, "api_key": os.getenv("SERPAPI_API_KEY")}
        try:
            search = GoogleSearch(params)
            results = search.get_dict()
            apps = [app["product_id"] for app in results["top_charts"]]
        except:
            apps = []
        all_apps.extend(apps)
        # print(chart, len(apps))
    return list(set(all_apps))


def get_section_links(section):
    links = section.query_selector_all("a")
    links = [link.get_attribute("href") for link in links if link.get_attribute("href")]
    links = [link for link in links if "details?id=" in link]
    return links


def get_app_ids(links):
    app_ids = []
    for link in links:
        app_id = link.split("id=")[-1].split("&")[0]
        app_ids.append(app_id)
    return app_ids


def scrape_category_apps(category: str, country: str):
    # https://serpapi.com/google-countries

    all_links = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()
        page.goto(f"https://play.google.com/store/apps/category/{category}?gl={country}")

        # Wait for the page to load
        page.wait_for_selector("section")

        sections = page.query_selector_all("section")
        for section in sections:
            buttons = section.query_selector_all("button[role='button']")
            if buttons:
                for button in buttons:
                    button.click()
                    page.wait_for_timeout(1000)

                    all_links.extend(get_section_links(section))
            else:
                all_links.extend(get_section_links(section))

        context.close()
        browser.close()

    apps = get_app_ids(all_links)
    apps = list(set(apps))
    return apps


def get_category_apps(category: str, country: str):
    # Added three sources - playwright scraping, serpapi charts, and search
    # since google play store only shows 40-50 apps for each chart (topselling - free/paid, top grossing)
    # could add serpapi search for reaching 500 apps
    scrape_results = scrape_category_apps(category, country)
    serp_results = serpapi_category_apps(category, country)
    search_results = search_apps(category, country)

    # combine both lists and get unique app ids
    app_ids = [*scrape_results, *serp_results, *search_results]
    app_ids = list(set(app_ids))
    return app_ids


def create_category_dataset(category: str, country: str, info=True):
    filename = f"dataset/dataset_{category}_{country}.json"
    app_ids = get_category_apps(category, country)

    dataset = []

    if info:
        for i, app_id in enumerate(app_ids):
            try:
                print(f"-----{i}-----")
                details = get_app_details(app_id)
                details = add_info(details)
                dataset.append(details)
            except Exception as e:
                print(f"XXXX {i} XXXX - {e}")
    else:
        dataset = app_ids

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(dataset, f, indent=4)
    return dataset
